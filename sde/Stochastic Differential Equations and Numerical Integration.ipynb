{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tutorial based on [An Algorithmic Introduction to Numerical Simulation of Stochastic Differential Equations][1] by Desmond J. Higham.\n",
    "\n",
    "[1]: https://doi.org/10.1137/S0036144500378302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "random.seed(0)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brownian Motion\n",
    "\n",
    "Definition of Brownian motion:\n",
    "\n",
    "1.  $W(0) = 0 \\quad .$\n",
    "2.  $W(t) - W(s) \\sim \\mathcal{N}\\left(0, \\sqrt{t-s}\\right) \\quad , \\quad 0 \\leq s < t \\leq T \\quad .$\n",
    "3.  $W(t) - W(s)$ and $W(v) - W(u)$, $0 \\leq s < t < u < v \\leq T$, are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brown(T, N, N_ens=1):\n",
    "    dt = T / N\n",
    "    sqrt_dt = np.sqrt(dt)\n",
    "\n",
    "    dW = random.normal(0., sqrt_dt, (N_ens, N))\n",
    "    W = np.zeros((N_ens, N + 1))\n",
    "    W[:, 1:] = np.cumsum(dW, axis=1)\n",
    "\n",
    "    return W, dW\n",
    "\n",
    "T = 1.\n",
    "N = 500\n",
    "t = np.linspace(0., 1., N + 1)\n",
    "\n",
    "N_ens = 10000\n",
    "W_ens, dW_ens = brown(T, N, N_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1)\n",
    "ax = fig.gca()\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$W$\")\n",
    "for k in range(100):\n",
    "    ax.plot(t, W_ens[k, :])\n",
    "    \n",
    "fig = plt.figure(2)\n",
    "ax = fig.gca()\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"standard deviation of $W$\")\n",
    "std_dev = np.sqrt(np.sum(W_ens**2, axis=0) / N_ens)\n",
    "ax.plot(t, np.sqrt(t), label=\"analytical\")\n",
    "ax.plot(t, std_dev, label=\"numerical\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Integrals\n",
    "\n",
    "We are going to approximate the stochastic integral $\\int_0^T{h(t)\\,\\mathrm{d}W}$ by a Riemann sum.\n",
    "In a stochastic differential equation, the term $h(t)\\,\\mathrm{d}W$ may represent stochastic forcing with Gaussian noise of time-dependent amplitude $h(t)$.\n",
    "The approximation of stochastic integrals by Riemann sums is not unique.\n",
    "Unlike for ordinary differential equations, the approximations are not equivalent as $\\delta t \\to 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Itô Integral\n",
    "\n",
    "The Itô integral is defined as the left-hand sum:\n",
    "\\begin{equation}\n",
    "\\int_0^T{h(t)\\,\\mathrm{d}W} = \\lim_{N \\to \\infty}{\\sum_{i=0}^{N-1}{h(t_i)\\left(W(t_{i+1}) - W(t_i)\\right)}} \\quad .\n",
    "\\end{equation}\n",
    "\n",
    "For $h(t) = W(t)$, the Itô integral gives:\n",
    "\\begin{align}\n",
    "\\sum_{i=0}^{N-1}{W(t_i)\\left(W(t_{i+1}) - W(t_i)\\right)}\n",
    "&= \\frac{1}{2}\\sum_{i=0}^{N-1}\\left[W(t_{i+1})^2 - W(t_i)^2 - \\left(W(t_{i+1}) - W(t_i)\\right)^2\\right] \\\\\n",
    "&= \\frac{1}{2}\\sum_{i=0}^{N-1}\\left[W(t_{i+1})^2 - W(t_i)^2\\right] - \\frac{1}{2}\\sum_{i=0}^{N-1}\\left(W(t_{i+1}) - W(t_i)\\right)^2 \\\\\n",
    "&= \\frac{1}{2}\\left[W(T)^2 - W(0)^2\\right] - \\frac{1}{2}\\sum_{i=0}^{N-1}\\left(W(t_{i+1}) - W(t_i)\\right)^2 \\quad , \\\\\n",
    "\\lim_{N \\to \\infty}{\\sum_{i=0}^{N-1}{W(t_i)\\left(W(t_{i+1}) - W(t_i)\\right)}}\n",
    "&= \\frac{1}{2}\\left[W(T)^2 - W(0)^2\\right] - \\frac{T}{2} \\quad .\n",
    "\\end{align}\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ito = np.sum(W_ens[:, :-1] * dW_ens, axis=1)\n",
    "\n",
    "fig = plt.figure(11)\n",
    "ax = fig.gca()\n",
    "ax.set_title(\"Itô integral\")\n",
    "ax.set_xlabel(r\"$W(T)$\")\n",
    "ax.set_ylabel(r\"$\\int_0^T{W(t)\\,\\mathrm{d}W(t)}$\")\n",
    "ax.plot(np.sort(W_ens[:, -1]), 0.5 * np.sort(W_ens[:, -1])**2 - 0.5 * T, label=\"analytical\")\n",
    "ax.scatter(W_ens[:, -1], ito, label=\"numerical\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratonovich Integral\n",
    "\n",
    "The Stratonovich integral is defined as the mid-point sum:\n",
    "\\begin{equation}\n",
    "\\int_0^T{h(t)\\,\\mathrm{d}W} = \\lim_{N \\to \\infty}{\\sum_{i=0}^{N-1}{h\\left(\\frac{t_i + t_{i+1}}{2}\\right)\\left(W(t_{i+1}) - W(t_i)\\right)}} \\quad .\n",
    "\\end{equation}\n",
    "\n",
    "The conditional probability distribution of $W\\left(\\frac{t_i + t_{i+1}}{2}\\right)$ given $W(t_i)$ and $W(t_{i+1})$ is given by\n",
    "\\begin{equation}\n",
    "W\\left(\\frac{t_i + t_{i+1}}{2}\\right) = \\frac{W(t_i) + W(t_{i+1})}{2} + \\Delta Z \\quad , \\quad \\Delta Z \\sim \\mathcal{N}\\left(0, \\frac{\\delta t}{4}\\right) \\quad .\n",
    "\\end{equation}\n",
    "For $h(t) = W(t)$, the Stratonovich integral gives:\n",
    "\\begin{align}\n",
    "\\sum_{i=0}^{N-1}{W\\left(\\frac{t_i + t_{i+1}}{2}\\right)\\left(W(t_{i+1}) - W(t_i)\\right)}\n",
    "&= \\sum_{i=0}^{N-1}{\\left[\\frac{W(t_i) + W(t_{i+1})}{2} + \\Delta Z\\right]\\left(W(t_{i+1}) - W(t_i)\\right)} \\\\\n",
    "&= \\frac{1}{2}\\sum_{i=0}^{N-1}\\left[W(t_{i+1})^2 - W(t_i)^2\\right] + \\Delta Z\\sum_{i=0}^{N-1}\\left(W(t_{i+1}) - W(t_i)\\right) \\\\\n",
    "&= \\frac{1}{2}\\left[W(T)^2 - W(0)^2\\right] + \\Delta Z\\left(W(T) - W(0)\\right) \\quad , \\\\\n",
    "\\lim_{N \\to \\infty}{\\sum_{i=0}^{N-1}{W\\left(\\frac{t_i + t_{i+1}}{2}\\right)\\left(W(t_{i+1}) - W(t_i)\\right)}}\n",
    "&= \\frac{1}{2}\\left[W(T)^2 - W(0)^2\\right] \\quad .\n",
    "\\end{align}\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat = np.sum(0.5 * (W_ens[:, :-1] + W_ens[:, 1:]) * dW_ens, axis=1) + random.normal(0, 0.25 * T / N, N_ens)\n",
    "\n",
    "fig = plt.figure(12)\n",
    "ax = fig.gca()\n",
    "ax.set_title(\"Stratonovich integral\")\n",
    "ax.set_xlabel(r\"$W(T)$\")\n",
    "ax.set_ylabel(r\"$\\int_0^T{W(t)\\,\\mathrm{d}W(t)}$\")\n",
    "ax.plot(np.sort(W_ens[:, -1]), 0.5 * np.sort(W_ens[:, -1])**2, label=\"analytical\")\n",
    "ax.scatter(W_ens[:, -1], strat, label=\"numerical\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The Euler-Maruyama Method\n",
    "\n",
    "A scalar, autonomous stochastic differential equation is given by\n",
    "\\begin{equation}\n",
    "\\mathrm{d}X = f(X(t))\\,\\mathrm{d}t + g(X(t))\\,\\mathrm{d}W \\quad ,\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "X(0) = X_0 \\quad .\n",
    "\\end{equation}\n",
    "If $g \\equiv 0$ and $X_0$ not a random variable, the problem becomes deterministic and reduces to an ordinary differential equation.\n",
    "\n",
    "The Euler-Maruyama method applies forward integration to the deterministic term $f(X(t))\\,\\mathrm{d}t$ and Itô integration to the stochastic term $g(X(t))\\,\\mathrm{d}W$:\n",
    "\\begin{equation}\n",
    "X_{i+1} = X(t_i) + f(X_i)\\,\\mathrm{d}t + g(X_i)\\left(W(t_{i+1}) - W(t_i)\\right) \\quad .\n",
    "\\end{equation}\n",
    "\n",
    "Example:\n",
    "\n",
    "We apply the Euler-Maruyama method to the linear stochastic differential equation\n",
    "\\begin{equation}\n",
    "\\mathrm{d}X = \\lambda X(t)\\,\\mathrm{d}t + \\mu X(t)\\,\\mathrm{d}W \\quad ,\n",
    "\\end{equation}\n",
    "where $\\lambda$ and $\\mu$ are constants.\n",
    "The exact solution is given by\n",
    "\\begin{equation}\n",
    "X(t) = X_0 \\exp\\left(\\left[\\lambda - \\frac{\\mu^2}{2}\\right] t + \\mu W(t)\\right) \\quad .\n",
    "\\end{equation}\n",
    "The expected value of the solution to the stochastic differential equation is given by:\n",
    "\\begin{equation}\n",
    "\\mathrm{d}(\\mathrm{E}[X]) = \\lambda \\mathrm{E}[X]\\,\\mathrm{d}t \\quad ,\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\mathrm{E}[X] = X_0 \\exp\\left(\\lambda t\\right) \\quad .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsde(T, N, N_ens, X_0, W, lamb, mu):\n",
    "    return X_0 * np.exp((lamb - 0.5 * mu**2) * np.linspace(0., T, N + 1) + mu * W)\n",
    "\n",
    "def lsde_em(T, N, N_ens, X_0, W, lamb, mu, stride=1):\n",
    "    N_e = N // stride\n",
    "    dt_e = T / N_e\n",
    "    \n",
    "    res = np.zeros((N_ens, N_e  + 1))\n",
    "    res[:, 0] = X_0\n",
    "    for i in range(N_e):\n",
    "        res[:, i + 1] = res[:, i] + lamb * res[:, i] * dt_e + mu * res[:, i] * (W[:, (i + 1) * stride] - W[:, i * stride])\n",
    "    \n",
    "    return res\n",
    "\n",
    "lamb = 2.\n",
    "mu = 1.\n",
    "em = lsde(T, N, N_ens, 1., W_ens, lamb, mu)\n",
    "\n",
    "em_1 = lsde_em(T, N, N_ens, 1., W_ens, lamb, mu)\n",
    "em_2 = lsde_em(T, N, N_ens, 1., W_ens, lamb, mu, 2)\n",
    "em_5 = lsde_em(T, N, N_ens, 1., W_ens, lamb, mu, 5)\n",
    "\n",
    "for i in range(3):\n",
    "    fig = plt.figure(20 + i)\n",
    "    ax = fig.gca()\n",
    "    ax.set_title(\"Euler-Maruyama method (sample {})\".format(i))\n",
    "    ax.set_xlabel(r\"$t$\")\n",
    "    ax.set_ylabel(r\"$X$\")\n",
    "    ax.plot(np.linspace(0., T, N + 1), em[i, :], label=\"analytical\")\n",
    "    ax.plot(np.linspace(0., T, N + 1), em_1[i, :], label=\"numerical (dt={})\".format(T / N))\n",
    "    ax.plot(np.linspace(0., T, N // 2 + 1), em_2[i, :], label=\"numerical (dt={})\".format(2 * T / N))\n",
    "    ax.plot(np.linspace(0., T, N // 5 + 1), em_5[i, :], label=\"numerical (dt={})\".format(5 * T / N))\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(30)\n",
    "ax = fig.gca()\n",
    "ax.set_title(\"Euler-Maruyama method (expected value)\".format(i))\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$X$\")\n",
    "ax.plot(np.linspace(0., T, N + 1), np.exp(lamb * np.linspace(0., T, N + 1)), label=\"analytical\")\n",
    "ax.plot(np.linspace(0., T, N + 1), np.mean(em, axis=0), label=\"analytical (average)\")\n",
    "ax.plot(np.linspace(0., T, N + 1), np.mean(em_1, axis=0), label=\"numerical (average, dt={})\".format(T / N))\n",
    "ax.plot(np.linspace(0., T, N // 2 + 1), np.mean(em_2, axis=0), label=\"numerical (average, dt={})\".format(2 * T / N))\n",
    "ax.plot(np.linspace(0., T, N // 5 + 1), np.mean(em_5, axis=0), label=\"numerical (average, dt={})\".format(5 * T / N))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong and Weak Convergence\n",
    "\n",
    "A stochastic method has strong order of convergence $\\gamma$ if\n",
    "\\begin{equation}\n",
    "\\mathrm{E}[||X_n - X(t_n)||] \\sim \\mathcal{O}((\\Delta t)^\\gamma) \\quad .\n",
    "\\end{equation}\n",
    "A stochastic method has weak order of convergence $\\alpha$ if\n",
    "\\begin{equation}\n",
    "||\\mathrm{E}[X_n] - \\mathrm{E}[X(t_n)]|| \\sim \\mathcal{O}((\\Delta t)^\\alpha) \\quad .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_t = [1 * T / N, 2 * T / N, 5 * T / N]\n",
    "\n",
    "error_strong = []\n",
    "error_strong.append(np.mean(np.abs(em_1 - em), axis=0)[-1])\n",
    "error_strong.append(np.mean(np.abs(em_2 - em[:, ::2]), axis=0)[-1])\n",
    "error_strong.append(np.mean(np.abs(em_5 - em[:, ::5]), axis=0)[-1])\n",
    "\n",
    "em_mean = np.mean(em, axis=0)\n",
    "error_weak = []\n",
    "error_weak.append(np.abs(np.mean(em_1, axis=0) - em_mean)[-1])\n",
    "error_weak.append(np.abs(np.mean(em_2, axis=0) - em_mean[::2])[-1])\n",
    "error_weak.append(np.abs(np.mean(em_5, axis=0) - em_mean[::5])[-1])\n",
    "\n",
    "fig = plt.figure(40)\n",
    "ax = fig.gca()\n",
    "ax.set_title(\"Stochastic convergence\")\n",
    "ax.set_xlabel(r\"$\\Delta t$\")\n",
    "ax.set_ylabel(r\"error\")\n",
    "ax.plot(delta_t, error_strong, label=\"strong\")\n",
    "ax.plot(delta_t, error_weak, label=\"weak\")\n",
    "ax.plot(delta_t, np.array(delta_t)**0.5 * error_strong[1] / delta_t[1]**0.5, label=r\"strong ($C (\\Delta t)^{0.5}$)\")\n",
    "ax.plot(delta_t, np.array(delta_t) * error_weak[1] / delta_t[1], label=r\"weak ($C \\Delta t$)\")\n",
    "ax.legend()\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
